### 这里先总结下之前的代码

- 如果伙伴们做到这里, 之前的代码一共有3块
1. 主函数
2. 训练
3. 验证结果

- 需要注意的是, 这些代码中, 大家不必在变量名上面和我保持一致, 尤其是那种3字母缩写, 我自己也吃尽了苦头......
- 下面是我的三块代码, 
 - 咱们的代码可能在变量命名上不一致, 
 - 也可能在代码风格上不一致, 比如你可能是class字面量, 也可能是class实例化new/self/继承这套走下来的.
 - 但是, 大体的逻辑思路应该是一致的.
 - 仔细观察, 大家可以看到主函数
  - 13行核心代码
   - 基础赋值3行定义ih,ho矩阵和acitive_f激活函数
   - 10行train函数
  - 6行辅助赋值代码
   - 基础4行
   - nn.query, nn.train
  - 5行重复代码
   - query函数
  - 无用代码一行
   - query的返回值
   - 上一行就可以返回
   - 甚至很多语言都可以省略这个返回
  - 有效代码量 13/13+6+5+1=52% 
   - 这个数字大大超越java
   - 但是少于lisp类语言



### 第一块: 主函数
```python
# 主函数
import numpy
import scipy.special
# 这里是主函数的定义, 定义为参数方式, 使用起来更方便
def nn(inputnode, hiddenode, outputnode, learnrate):
    # 定义参数--------------
    nn.inputnode=inputnode # 输入层网络节点数量
    nn.hiddenode=hiddenode # 隐藏层节点数量
    nn.outputnode=outputnode # 输出层节点数量
    nn.learnrate=learnrate # 这个比率是学习的速率

    # 正态分布初始化
    nn.ih = numpy.random.normal(0.0, pow(nn.hiddenode, -0.5),(nn.hiddenode, nn.inputnode))
    nn.ho = numpy.random.normal(0.0, pow(nn.outputnode, -0.5),(nn.outputnode, nn.hiddenode))
    # 启动函数 activation function 就是节点运算的函数
    nn.active_f=lambda x: scipy.special.expit(x)
    # 查询函数, 参数是一个list(数组)
    def query(inpl):
        # ----------这里是我偷偷插入的一行, 没什么可讲的, 就是把一维数组(向量), 整理成为二维矩阵
        inp=numpy.array(inpl, ndmin=2).T
        # 矩阵乘法如此简单
        hli=numpy.dot(nn.ih, inp)   # hidden level input
        hlo = nn.active_f(hli) # hidden level output 隐藏层输出等于用s函数处理一下输入
        # 输出层, 一毛一样
        oli=numpy.dot(nn.ho, hlo)
#         print('oli',oli)
        olo = nn.active_f(oli)
        return olo
    nn.query=query
    def train(inpl,tarl):
        inp=numpy.array(inpl, ndmin=2).T
        hli=numpy.dot(nn.ih, inp)   
        hlo = nn.active_f(hli) 
        oli=numpy.dot(nn.ho, hlo)   
        olo = nn.active_f(oli) 
        # 增加了目标数组
        tar=numpy.array(tarl, ndmin=2).T
        ## 拿到误差
        output_error=tar -olo # 最终结果误差
        # python做矩阵减法, 非常直观
        hidden_error=numpy.dot(nn.ho.T, output_error) 
        # hidden层的误差, 这里又是一次矩阵乘法
        ## 修正权重
        nn.ho += nn.learnrate*numpy.dot( output_error*(olo*(1-olo)), numpy.transpose(hlo))
        # 隐藏层到输出层的权重矩阵 += 学习速率* 矩阵乘法, 这个矩阵乘法的解释如下: 
        # olo*(1-olo)这个是一个S函数, 然后作用于输出误差矩阵,
        nn.ih += nn.learnrate*numpy.dot( hidden_error*(hlo*(1-hlo)), numpy.transpose(inp))
    nn.train=train
```
### 第二块: 训练
```python
# --------------训练-------------
# 打开文件, 这个文件有十条数据, 文件在群里面, 把它放到你的python notebook所在的目录
dfile=open("./mnist_train_100.csv",'r')
# 读取数据
dlist=dfile.readlines()
# 关闭文件
dfile.close()
# 让参数生效
nn(28*28, 100, 10, 0.3)
# 训练所有数据
for re in dlist:
    a=re.split(',') # 把字符串用逗号拆成数组
    # 这里是把颜色的数字压缩到0-1之间
    ia=(numpy.asfarray(a[1:])/255.0*0.99+0.01)
    # 制造训练的target数据
    targetv=numpy.zeros(nn.outputnode) +0.01 # 整体都初始化为0.01
    # 因为第一个数字是这一行数据的数字, 所以用这个数字做target是非常合理的
    targetv[int(a[0])]=0.99 # 只有目标的值是0.99, 其他值应该都是0.01
    # 开始训练
    nn.train(ia, targetv)
```

### 第三块: 验证结果
```python
#----------验证结果-----------
dfile=open("./mnist_test_10.csv",'r')
dlist=dfile.readlines()
dfile.close()
import matplotlib.pyplot
a=dlist[0].split(',')# 这里dlist[0]代表第一条, 可以换换其他数字试试
print(a[0]) # 看一下这个数组的第一个元素, 这个元素是这个数组实际的数字值
image_a=numpy.asfarray(a[1:]).reshape((28,28))
matplotlib.pyplot.imshow(image_a, cmap='Greys',interpolation='None')
# 测试一下这一条数据
ia=(numpy.asfarray(a[1:])/255.0*0.99+0.01)
nn.query(ia) 
```