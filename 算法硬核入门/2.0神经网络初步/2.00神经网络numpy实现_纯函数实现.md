> 用numpy手撸一个神经网络
### 纯函数手撸
- 先定义整体结构, 一般的机器学习都是这三部分

```python
# 整体结构
# 主函数
def nn():
    # 训练函数
    def train():
        return 'train在这里'
    # 查询函数
    def query():
        return 'query在这里'
    nn.train=train
    nn.query=query

# 此时可以用下面代码测试这个啥也干不了的框架
nn()
nn.train()
```


> 别怕, 代码拢共2步
#### 1/2 初始框架

- 新开一个cell, 逐个尝试下面代码

```python
# 定义参数--------------
nn.inputnode=3 # 输入层网络节点数量
nn.hiddenode=3 # 隐藏层节点数量
nn.outputnode=3 # 输出层节点数量

import numpy
# 定义权重矩阵--------------
# 拿到一个随机矩阵
a=numpy.random.rand(3,3)
a

# 减0.5再看看
a=numpy.random.rand(3,3)-0.5
a

import matplotlib.pyplot
# 可视化看看
matplotlib.pyplot.imshow(a)

# 设置一个
nn.ih = numpy.random.rand(nn.hiddenode,nn.inputnode)-0.5
nn.ho = numpy.random.rand(nn.outputnode,nn.hiddenode)-0.5


# 可选项: 更复杂的统计学定义
# 正态分布初始化
nn.ih = numpy.random.normal(0.0, pow(nn.hiddenode, -0.5),(nn.hiddenode, nn.inputnode))
nn.ho = numpy.random.normal(0.0, pow(nn.outputnode, -0.5),(nn.outputnode, nn.hiddenode))
        

# ----------此处插入矩阵和神经网络的关系----------------
# 矩阵乘法如此简单
hli=numpy.dot(nn.ih, inp)   # hidden level input



# 启动函数 activation function 就是节点运算的函数
import scipy.special
nn.active_f=lambda x: scipy.special.expit(x)
# 这里lambda是一种快捷函数定义, 它很简便优雅, 但是, 只能定义一行函数, 冒号前面是参数, 后面是返回值, 这玩意有lisp内味了.

hlo = nn.active_f(hli) # hidden level output 隐藏层输出等于用s函数处理一下输入

# 这样隐藏层就处理好了.
# 输出层也是同样的处理方式, 同学们自己思考下......

```
- ================================================
- 代码运行到这里, 伙伴们都理解了他的方法和意义, 那么现在把这些方法都放到函数里面:  
- ================================================

```python
# 主函数
import numpy
import scipy.special
# 这里是主函数的定义, 定义为参数方式, 使用起来更方便
def nn(inputnode, hiddenode, outputnode, learnrate):
    # 定义参数--------------
    nn.inputnode=inputnode # 输入层网络节点数量
    nn.hiddenode=hiddenode # 隐藏层节点数量
    nn.outputnode=outputnode # 输出层节点数量
    # 正态分布初始化
    nn.ih = numpy.random.normal(0.0, pow(nn.hiddenode, -0.5),(nn.hiddenode, nn.inputnode))
    nn.ho = numpy.random.normal(0.0, pow(nn.outputnode, -0.5),(nn.outputnode, nn.hiddenode))
    # 启动函数 activation function 就是节点运算的函数
    nn.active_f=lambda x: scipy.special.expit(x)
    nn.learnrate=learnrate # 这个比率是学习的速率
    # 查询函数, 参数是一个list(数组)
    def query(inpl):
        # ----------这里是我偷偷插入的一行, 没什么可讲的, 就是把一维数组(向量), 整理成为二维矩阵
        inp=numpy.array(inpl, ndmin=2).T
        # 矩阵乘法如此简单
        hli=numpy.dot(nn.ih, inp)   # hidden level input
        hlo = nn.active_f(hli) # hidden level output 隐藏层输出等于用s函数处理一下输入
        # 输出层, 一毛一样
        oli=numpy.dot(nn.ho, hlo)
        olo = nn.active_f(oli)
        return olo
    nn.query=query
```
- 运行一下试试
```python
nn(3,5,2,0.5)
nn.query([1.0,0.5,-1.5])
# 可以尝试下下面这一句
query([1.0,0.5,-1.5]) # 这个可能报错, 参见下面的解释
# 这个结果可能让伙伴们大吃一惊, 难道python没有闭包吗?. 难道函数是全局定义的吗? 并不是, 很可能是缓存引起的, del query之后再执行就不可以了. 只能用下面这种方式
```
> 运行一下试试, 多运行几次, 会发现每次结果都不同, 仔细想想为什么?  
> -- 提示, 如果想不通, 就注释一些代码试试, 也可以print中间变量看看情况

#### 2/2 训练网络
> 这个段落出乎意料的简单....... 其实这才揭示了机器学习是非常简单的内容
> 可以在之前预留的train函数的位置写以下代码(喜欢新建一个cell 单独写以下代码, 运行OK之后再搬运会]回预留的function train位置, 也是可以的)
```python
# 首先, 可以把query复制过来, 并且把名字改为train
    def train(inpl,tarl):
        # -----和query一毛一样的5行代码------
        inp=numpy.array(inpl, ndmin=2).T
        hli=numpy.dot(nn.ih, inp)   
        hlo = nn.active_f(hli) 
        oli=numpy.dot(nn.ho, hlo)   
        olo = nn.active_f(oli) 
        # ------从这里开始是不一样的代码了------------
        # 增加了目标数组
        tar=numpy.array(tarl, ndmin=2).T
        # 因为是训练函数, 所以, 我们是有预期结果的, 这种有预期结果的训练就叫: 有监督训练
        # nn.learnrate=0.5 # 之前设置的这个比率真的用在了这里, 这个是学习的速率

        # 运算结果就需要做反向传播调整了
        ## 拿到误差
        output_error=tar -olo # 最终结果误差
        # python做矩阵减法, 非常直观

        hidden_error=numpy.dot(nn.ho.T, output_error) 
        # hidden层的误差, 这里又是一次矩阵乘法

        ## 修正权重
        nn.ho += nn.learnrate*numpy.dot( output_error*(olo*(1-olo)), numpy.transpose(hlo))
        # 还记得吗? nn.learnrate 就是一开始设置的学习比率, 这里才用到
        # 主运算符 a+=b 和其他语言一致, 代表 a=a+b, 意思就是隐藏到输出的那个矩阵做一次加法, 对自身进行修正
        # 隐藏层到输出层的权重矩阵 += 学习速率* 矩阵乘法, 这个矩阵乘法的解释如下: 
        # olo*(1-olo)这个是一个S函数, 然后作用于输出误差矩阵,
        # hlo要进行转置才能和output_error结果误差做矩阵乘法(点乘)

        # 如果理解了上面那一句, 那么下面是输入到隐藏层的权重调整, 几乎一毛一样
        nn.ih += nn.learnrate*numpy.dot( hidden_error*(hlo*(1-hlo)), numpy.transpose(inp))
    nn.train=train
```
> 大功告成, 代码都完成了.

### 用训练数据测试这个神经网络
> 咱们使用手写数字作为目标, 目标是这个神经网络可以识别出手写数字
- 引入训练数据
```python
# --------------训练-------------
# 打开文件, 这个文件有十条数据, 文件在群里面, 把它放到你的python notebook所在的目录
dfile=open("./mnist_train_100.csv",'r')
# 读取数据
dlist=dfile.readlines()
# 关闭文件
dfile.close()
# 绘制一下数据, 直观看一下------
# 引入绘图包
import matplotlib.pyplot
# 读第一条数据, 并且格式化成为数组(list)
a=dlist[0].split(',')# 这里dlist[0]代表第一条, 可以换换其他数字试试
print(a[0]) # 看一下这个数组的第一个元素, 这个元素是这个数组实际的数字值
# 除了第一个数据之外的数据格式化为28*28的矩阵
image_a=numpy.asfarray(a[1:]).reshape((28,28))
# 此处a[1:]是一个切片, 更一般的写法是a[b:c], 意思是a数组中, 从b到c的那些元素, b和c如果是起始点或者终点, 则可以省略
matplotlib.pyplot.imshow(image_a, cmap='Greys',interpolation='None')
# 这一句话, 就是把这个数组画出来, 按照28*28个像素画出来
# 此时需要调整一波参数
# nn.inputnode=28*28 # 输入层网络节点数量, 这个要和输入数据匹配, 我们的输入每一条都是28*28
# nn.hiddenode=100 # 这个是按照经验设置的, 咱们要识别手写数字, 太小的节点量肯定不好, 太大的节电量会导致计算很慢. 这个也是尝试出来的
# nn.outputnode=10 # 因为咱们就识别0-9的数字, 因此, 输出10是很合理的, 理想情况, 其中一个是0.99, 另外几个都是0.01
# nn.learnrate=0.3 # 学习速率, 一般而言越小越好, 但是, 越小计算量越大
# 让参数生效
nn(28*28, 100, 10, 0.3)
# 训练所有数据
for re in dlist:
    a=re.split(',') # 把字符串用逗号拆成数组
    # 这里是把颜色的数字压缩到0-1之间
    ia=(numpy.asfarray(a[1:])/255.0*0.99+0.01)
    # 制造训练的target数据
    targetv=numpy.zeros(nn.outputnode) +0.01 # 整体都初始化为0.01
    # 因为第一个数字是这一行数据的数字, 所以用这个数字做target是非常合理的
    targetv[int(a[0])]=0.99 # 只有目标的值是0.99, 其他值应该都是0.01
    # 开始训练
    nn.train(ia, targetv)
# 好了, 可以测试运行一下, 看看是否有啥拼写错误
# 此时应该没有任何问题
# 我们马上进入激动人心的下一步, 检测我们网络的结果
```

### 训练网络的最终结果
- 新开一个notebook段落, 执行下面的代码, 别怕, 全部都是和以前一样的代码
```python
#----------验证结果-----------
dfile=open("./mnist_test_10.csv",'r')
dlist=dfile.readlines()
dfile.close()
import matplotlib.pyplot
a=dlist[0].split(',')# 这里dlist[0]代表第一条, 可以换换其他数字试试
print(a[0]) # 看一下这个数组的第一个元素, 这个元素是这个数组实际的数字值
image_a=numpy.asfarray(a[1:]).reshape((28,28))
matplotlib.pyplot.imshow(image_a, cmap='Greys',interpolation='None')
# 测试一下这一条数据
ia=(numpy.asfarray(a[1:])/255.0*0.99+0.01)
nn.query(ia) 
# ----------执行上面这段代码------------
# ----------见证奇迹的时刻到了--------------

```
