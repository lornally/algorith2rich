> 预习目标: 用numpy手撸一个神经网络
> 课前练习, 不求完全理解, 追求的是一个整体的认知
> 代码调通之后, 请务必花时间回顾一下, 看看代码是如何运转的
> 因为, 调用了各种库函数, 因此, 如果不参考任何内容, 大家是无法徒手写出来的. 这也是算法工程师日常的工作状态, 大家总是需要查询各种库的功能.
> 本教程的目标是有了整体了解后, 大家可以明白自己需要阅读哪些库.
### 纯函数手撸
- 先定义整体结构, 一般的机器学习都是这三部分

```python
# 整体结构
# 主函数
def nn():
    # 训练函数
    def train():
        return 'train在这里'
    # 查询函数
    def query():
        return 'query在这里'
    nn.train=train
    nn.query=query

# 此时可以用下面代码测试这个啥也干不了的框架
nn()
nn.train()
```


> 别怕, 代码拢共2步
#### 1/2 初始框架

- 新开一个cell, 逐个尝试下面代码

```python
# 定义参数--------------
# 此处是神经网络的三个层, 输入层, 隐藏层, 输出层, 他们实际的节点数不一定, 但是, 此处为了简单, 都是3
nn.inputnode=3 # 输入层网络节点数量
nn.hiddenode=3 # 隐藏层节点数量
nn.outputnode=3 # 输出层节点数量

import numpy
# 定义权重矩阵--------------
# 拿到一个随机矩阵
a=numpy.random.rand(3,3)
a

# 减0.5再看看
a=numpy.random.rand(3,3)-0.5
a

import matplotlib.pyplot
# 可视化看看
matplotlib.pyplot.imshow(a)

# 设置一个
# nn.ih是nn的一个属性, 目标是定义从输入层到到隐藏层的系数矩阵, 数据到隐藏层需要用这个矩阵做计算
# 因为原始的随机数0-1, 用减法调整值域为-0.5到到+0.5
nn.ih = numpy.random.rand(nn.hiddenode,nn.inputnode)-0.5
nn.ho = numpy.random.rand(nn.outputnode,nn.hiddenode)-0.5


# 可选项: 更复杂的统计学定义
# 正态分布初始化, 简单的说就是让随机性更符合现实一般规律, 正态的函数是numpy.random.normal(0.0,a, (b,c)), a是实数, b,c是矩阵维数
nn.ih = numpy.random.normal(0.0, pow(nn.hiddenode, -0.5),(nn.hiddenode, nn.inputnode))
nn.ho = numpy.random.normal(0.0, pow(nn.outputnode, -0.5),(nn.outputnode, nn.hiddenode))
        

# ----------此处插入矩阵和神经网络的关系----------------
# 矩阵乘法如此简单(矩阵1乘矩阵2), numpy.dot(矩阵一, 矩阵二)
hli=numpy.dot(nn.ih, inp)   # inp是输入矩阵, 这个结果是hidden level input



# 启动函数 activation function 就是节点运算的函数
import scipy.special
# 启动函数根据需要可以使用不同的各种函数, 此处使用的是s函数(也叫sigma函数), 就是平滑跃迁函数, 详情可以自行看: https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.expit.html
nn.active_f=lambda x: scipy.special.expit(x)
# 这里lambda是一种快捷函数定义, 它很简便优雅, 但是, 只能定义一行函数, 冒号前面是参数, 后面是返回值, 这玩意有lisp内味了.

hlo = nn.active_f(hli) # hidden level output 隐藏层输出等于用s函数处理一下输入

# 这样隐藏层就处理好了.
# 输出层也是同样的处理方式, 同学们自己思考下......

```
- ================================================
- 代码运行到这里, 伙伴们都理解了他的方法和意义, 那么现在把这些方法都放到函数里面:  
- ================================================

```python
# 主函数
import numpy
import scipy.special
# 这里是主函数的定义, 定义为参数方式, 使用起来更方便
def nn(inputnode, hiddenode, outputnode, learnrate):
    # 定义参数--------------
    nn.inputnode=inputnode # 输入层网络节点数量
    nn.hiddenode=hiddenode # 隐藏层节点数量
    nn.outputnode=outputnode # 输出层节点数量
    # 正态分布初始化
    nn.ih = numpy.random.normal(0.0, pow(nn.hiddenode, -0.5),(nn.hiddenode, nn.inputnode))
    nn.ho = numpy.random.normal(0.0, pow(nn.outputnode, -0.5),(nn.outputnode, nn.hiddenode))
    # 启动函数 activation function 就是节点运算的函数
    nn.active_f=lambda x: scipy.special.expit(x)
    nn.learnrate=learnrate # 这个比率是学习的速率
    # 查询函数, 参数是一个list(数组)
    # 未来调用: query([1.0,0.5,-1.5])
    def query(inpl): # inpl是输入向量
        # ----------这里是我偷偷插入的一行, 没什么可讲的, 就是把一维数组(向量), 整理成为二维矩阵
        inp=numpy.array(inpl, ndmin=2).T
        # 矩阵乘法如此简单
        hli=numpy.dot(nn.ih, inp)   # hidden level input
        hlo = nn.active_f(hli) # hidden level output 隐藏层输出等于用s函数处理一下输入
        # 输出层, 一毛一样
        oli=numpy.dot(nn.ho, hlo)
        olo = nn.active_f(oli)
        return olo
    nn.query=query
    # 此句保证可以这样调用: nn.query([1.0,0.5,-1.5])
```
- 运行一下试试
```python
nn(3,5,2,0.5)
nn.query([1.0,0.5,-1.5])
# 可以尝试下下面这一句
query([1.0,0.5,-1.5]) # 这个可能报错, 参见下面的解释
# 这个结果可能让伙伴们大吃一惊, 难道python没有闭包吗?. 难道函数是全局定义的吗? 并不是, 很可能是缓存引起的, del query之后再执行就不可以了. 只能用下面这种方式
```
> 运行一下试试, 多运行几次, 会发现每次结果都不同, 仔细想想为什么?  
> -- 提示, 如果想不通, 就注释一些代码试试, 也可以print中间变量看看情况

#### 2/2 训练网络
> 这个段落出乎意料的简单....... 其实这才揭示了机器学习是非常简单的内容
> 可以在之前预留的train函数的位置写以下代码(喜欢新建一个cell 单独写以下代码, 运行OK之后再搬运会]回预留的function train位置, 也是可以的)
```python
# 首先, 可以把query复制过来, 并且把名字改为train
# 调用方式:  nn.train(ia, targetv)
    def train(inpl,tarl): # inpl是输入向量, tarl是校验向量
        # -----和query一毛一样的5行代码------
        inp=numpy.array(inpl, ndmin=2).T
        hli=numpy.dot(nn.ih, inp)   
        hlo = nn.active_f(hli) 
        oli=numpy.dot(nn.ho, hlo)   
        olo = nn.active_f(oli) 
        # ------从这里开始是不一样的代码了------------
        # 增加了目标数组
        tar=numpy.array(tarl, ndmin=2).T
        # 因为是训练函数, 所以, 我们是有预期结果的, 这种有预期结果的训练就叫: 有监督训练
        # nn.learnrate=0.5 # 之前设置的这个比率真的用在了这里, 这个是学习的速率

        # 运算结果就需要做反向传播调整了
        ## 拿到误差
        output_error=tar -olo # 最终结果误差
        # python做矩阵减法, 非常直观

        hidden_error=numpy.dot(nn.ho.T, output_error) 
        # hidden层的误差, 这里又是一次矩阵乘法

        ## 修正权重
        nn.ho += nn.learnrate*numpy.dot( output_error*(olo*(1-olo)), numpy.transpose(hlo))
        # 还记得吗? nn.learnrate 就是一开始设置的学习比率, 这里才用到
        # 主运算符 a+=b 和其他语言一致, 代表 a=a+b, 意思就是隐藏到输出的那个矩阵做一次加法, 对自身进行修正
        # 隐藏层到输出层的权重矩阵 += 学习速率* 矩阵乘法, 这个矩阵乘法的解释如下: 
        # olo*(1-olo)这个是一个S函数, 然后作用于输出误差矩阵, 还记得吗? 启动函数active_f用的也是他
        # hlo要进行转置才能和output_error结果误差做矩阵乘法(点乘)

        # 如果理解了上面那一句, 那么下面是输入到隐藏层的权重调整, 几乎一毛一样
        nn.ih += nn.learnrate*numpy.dot( hidden_error*(hlo*(1-hlo)), numpy.transpose(inp))
    nn.train=train
```
> 大功告成, 代码都完成了.

### 用训练数据测试这个神经网络
> 咱们使用手写数字作为目标, 目标是这个神经网络可以识别出手写数字
- 引入训练数据
```python
# --------------训练-------------
# 打开文件, 这个文件有十条数据, 文件在群里面, 把它放到你的python notebook所在的目录
dfile=open("./mnist_train_100.csv",'r')
# 读取数据, readlines函数按照[行]读入所有数据
dlist=dfile.readlines()
# 关闭文件
dfile.close()
# 绘制一下数据, 直观看一下------
# 引入绘图包
import matplotlib.pyplot
# 读第一条数据, 并且格式化成为数组(list)
a=dlist[0].split(',')# 这里dlist[0]代表第一条, 可以换换其他数字试试
print(a[0]) # 看一下这个数组的第一个元素, 这个元素是这个数组实际的数字值
# 除了第一个数据之外的数据格式化为28*28的矩阵
image_a=numpy.asfarray(a[1:]).reshape((28,28))
# 此处a[1:]是一个切片, 更一般的写法是a[b:c], 意思是a数组中, 从b到c的那些元素, b和c如果是起始点或者终点, 则可以省略
matplotlib.pyplot.imshow(image_a, cmap='Greys',interpolation='None')
# 这一句话, 就是把这个数组画出来, 按照28*28个像素画出来
# 此时需要调整一波参数
# nn.inputnode=28*28 # 输入层网络节点数量, 这个要和输入数据匹配, 我们的输入每一条都是28*28
# nn.hiddenode=100 # 这个是按照经验设置的, 咱们要识别手写数字, 太小的节点量肯定不好, 太大的节电量会导致计算很慢. 这个也是尝试出来的
# nn.outputnode=10 # 因为咱们就识别0-9的数字, 因此, 输出10是很合理的, 理想情况, 其中一个是0.99, 另外几个都是0.01
# nn.learnrate=0.3 # 学习速率, 一般而言越小越好, 但是, 越小计算量越大
# 让参数生效
nn(28*28, 100, 10, 0.3)
# 训练所有数据
for re in dlist:
    a=re.split(',') # 把字符串用逗号拆成数组
    # 这里是把颜色的数字压缩到0-1之间, 颜色数值一般是0-255, 如果不缩数字持续乘法会导致过大, 大到超过计算机能处理的极限
    ia=(numpy.asfarray(a[1:])/255.0*0.99+0.01)
    # 制造训练的target数据
    targetv=numpy.zeros(nn.outputnode) +0.01 # 整体都初始化为0.01
    # 因为第一个数字是这一行数据的数字, 所以用这个数字做target是非常合理的
    targetv[int(a[0])]=0.99 # 只有目标的值是0.99, 其他值应该都是0.01
    # 开始训练
    nn.train(ia, targetv)
# 好了, 可以测试运行一下, 看看是否有啥拼写错误
# 此时应该没有任何问题
# 我们马上进入激动人心的下一步, 检测我们网络的结果
```

### 训练网络的最终结果
- 新开一个notebook段落, 执行下面的代码, 别怕, 全部都是和以前一样的代码
```python
#----------验证结果-----------
dfile=open("./mnist_test_10.csv",'r')
dlist=dfile.readlines()
dfile.close()
import matplotlib.pyplot
a=dlist[0].split(',')# 这里dlist[0]代表第一条, 可以换换其他数字试试
print(a[0]) # 看一下这个数组的第一个元素, 这个元素是这个数组实际的数字值
image_a=numpy.asfarray(a[1:]).reshape((28,28))
matplotlib.pyplot.imshow(image_a, cmap='Greys',interpolation='None')
# 测试一下这一条数据
ia=(numpy.asfarray(a[1:])/255.0*0.99+0.01)
nn.query(ia) 
# ----------执行上面这段代码------------
# ----------见证奇迹的时刻到了--------------

```

### 思考内容:
1. 简单网络有几部分构成?
2. 神经网络的训练需要哪些组成部分? 
3. 咱们都使用了哪些库? 其中哪些是工作必须的, 哪些是辅助性的?
4. 请大致阅读以下库内容, 看一下简介和wiki即可:
    - numpy, 尤其是矩阵计算内容
    - scipy, 想一下s(sigma)函数如此简单, 为啥要用scipy.special?
    - matplotlib, 这个库的功能是辅助功能, 但是也是python的核心竞争力
    - pandas, 这个库咱们本次没有用到, 但是, 非常重要
5. python的核心竞争力是完善的各种库, 你认同这句话吗? 为什么?