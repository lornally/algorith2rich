neural networks


### DNN
- 全连接深度网络, 甚至可以>100层
- 如果是狭义的定义, 那么等价于MLP 多层感知器, feed forward network
- 关键问题: 
 - 是要避免梯度消失, 由此避免陷入局部最优
 - 参数膨胀, 输入层就是图像的像素, 隐藏层要更大
 - 无法对时间序列建模
- 高速公路网络（highway network）
- 深度残差学习（deep residual learning）


### CNN
- 卷积神经网络convolutional 
- 解决问题: 参数膨胀的问题, 使用卷积核

### RNN
- 循环神经网络 recurrent
- 解决问题: 时间序列建模, 
- 自身会循环处理自身的上一次(t-1时刻)输出
- 但是依旧会有随着时间进展梯度消失的问题, 久远之前的输出已经无法对后续的训练形成影响
- 解决: LSTM
- 双向循环, 训练的时候有正向, 还有反向从t+1时刻的输出, 去训练t时刻的网络层
- 递归神经网络(树结构)recursive



## ----- 可以无监督 =======

### DGM 深度生成模型 deep generative
- 玻尔兹曼机 boltzmann machine
- 受限玻尔兹曼机 restricted boltzmann

### DBNN 深度信念网络 deep belef
- 堆叠的受限玻尔兹曼机 stacked RBM


### GAN 生成对抗网络 generative adversarial
- 变分自编码器 variational auto encoder


### 自编码器 auto encoder
- 编码
- 解码
- 图像 -> 编码 -> 压缩表示 -> 解码
- 中间这个压缩表示, 就是精华