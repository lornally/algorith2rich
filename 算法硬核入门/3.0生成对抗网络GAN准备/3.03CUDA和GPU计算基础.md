### numpy和矩阵乘法
- 重温一下numpy的矩阵乘法和python自己的矩阵乘法

```python
import numpy

s=600
a = numpy.random.rand(s,s)
b = numpy.random.rand(s,s)

# 新建一个cell执行下面代码
%%timeit
c = numpy.dot(a, b)

# 新建一个cell, 直接写python乘法
%%timeit
c= numpy.zeros.(s,s)
for i in range(s):
 for j in range(s):
  for k in range(s):
   c[i,j] += a[i,k]*b[k,j]

# 此时发现numpy快很多, 原因是numpy利用了cpu特性做了优化, 例如: 寄存器/缓存/内存/并行计算单元
```

### 使用CUDA
- 很多时候, numpy也不够快, 因此要用cuda, 这玩意是显卡特性, google还有更猛的专为机器学习设计的tpu
- 菜单选择
 - runtime
 - change runtime type
- 然后可以愉快地选择
 - GPU
 - TPU
> 特别注意: google每天只能用12个小时的GPU, 因此, 写好代码再换执行, 执行之后, 就切回去, 避免, 需要GPU却没得用的尴尬境地

```python
import torch
# 正常张量
x=torch.FloatTensor([3.5])
# 使用GPU
x=torch.cuda.FloatTensor([3.5])

# 看下x的类型
print(x.type())
x.device

# 此时执行一下代码, 大家会发现不使用GPU就会报错: 找不到GPU``````````````````
```
- pytorch矩阵乘法
```python
%%timeit
import torch
aa=torch.FloatTensor(a)
bb=torch.FloatTensor(b)
cc=torch.matmul(aa,bb)


### gpu版本
%%timeit
import torch
aa=torch.cuda.FloatTensor(a)
bb=torch.cuda.FloatTensor(b)
cc=torch.matmul(aa,bb)
```

- 标准的cuda检查代码
```python
import torch
if torch.cuda.is_available():
 torch.set_default_tensor_type(torch.cuda.FloatTensor)
 print('use:', torch.cuda.get_device_name(0))
 pass

device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device


```

