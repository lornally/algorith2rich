

### pytorch functional 入门
- 老接口: torch.nn.module
- 新接口: torch.nn.functional



### 神经网络代码

```python
# 主函数
import torch
import torch.nn as nn

# ! 这里是主函数的定义, 为了闭包数据, 避免影响全局环境
def fnn():
  # 生成一个module, 它是torch的基础神经网络对象
  mnn=nn.Module()
  # 定义基础参数
  mnn.model=nn.Sequential(
      nn.Linear(784,200), # 线性全连接从输入到中间 ax+b, 这里a和b都是学习会调整的参数, 此处已经比我们numpy的实现强大了一丢丢
      nn.Sigmoid(), # 中间层的激活函数
      nn.Linear(200,10), # 线性全连接从中间到输出
      nn.Sigmoid(), # 输出层的激活函数
  )

  # 优化设计, SGD是随机梯度下降stochastic gradient descent, lr就是学习率0.01(注意咱们numpy版本最小也是0.1, 此处学习率已经提升了10倍精度)

  # 定义损失函数
  mnn.loss_function=nn.MSELoss()
  mnn.optimiser=torch.optim.SGD(mnn.parameters(), lr=0.01)

  # 这里是验证函数
  def forward(inputs):
    # 非常简便, 直接调用官方提供的model函数就能实现验证功能
    return mnn.model(inputs)
  fnn.forward=forward

  # 定义train之前, 需要定义几个属性
  mnn.counter=0 # 可视化训练需要的计数器
  mnn.progress=[] # 可视化训练需要的过程中的参数, 存储参数来自于下面定义的损失函数
  # 定义损失函数, 此处使用均方误差, mean squared error
  mnn.loss_function=nn.MSELoss()
  # 训练函数
  def train(inp,targ):
    # 第一步, 训练拿到模型结果
    outp=fnn.forward(inp)
    # 计算损失值, 定义loss, 这里使用了之前定义的loss_function
    loss=mnn.loss_function(outp, targ)
    # ! 这三行是精华
    mnn.optimiser.zero_grad() # 将计算图中的梯度归零, 避免每次计算叠加效果, 换句话说, 很多机器学习算法会需要叠加每一轮的梯度
    loss.backward() # loss计算了梯度
    mnn.optimiser.step() # 更新网络的可学习参数
    # 此时代码已经完成了我们在numpy方式中做的所有功能
    # 下面的内容是为了可视化训练, 在训练中, 我们能够监控训练情况
    mnn.counter +=1
    if(mnn.counter%10==0):
      # 这一行item()函数是为了取值方便, progress是之前定义的数组
      mnn.progress.append(loss.item())
      if(mnn.counter%10000==0):
        print('counter', mnn.counter)
  fnn.train=train
  # 绘图函数, 图示训练过程
  def plot_progress():
    # 这里用了pandas, 定义了数据来源, 以及列的表头使用哪一行
    df=pandas.DataFrame(mnn.progress, columns=['lose'])
    # 这里调整视图的各种格式风格参数, 大伙先用吧, 如果想要了解pandas, 这个本身也需要一次分享, 在此之前, 只要知道pandas是干翻了R的库, 就可以了
    df.plot(ylim=(0,1.0), figsize=(16,8), alpha=0.1, marker='.',grid=True, yticks=(0,0.25,0.5))
  fnn.plot_progress=plot_progress

```

### 准备数据

#### 载入数据
```python
# ! 加载googlo盘, 拿数据
from google.colab import drive
drive.mount('./mount')
```
- 运行这个cell, 会出现授权界面, 一定要授权成功

#### pandas准备
> pandas是比较重要的内容, 并且这里咱们会使用python官方推荐的替代iterable的class定义的写法yield, 因此, 咱们要一步一步来

```python
# 1. 咱们先看一下数据的情况
<<<<<<< Updated upstream
<<<<<<< Updated upstream
=======
=======
import pandas
import numpy
import torch
import torch.nn as nn
from torch.utils.data import Dataset
import matplotlib.pyplot as plt
df=pandas.read_csv('mount/My Drive/Colab Notebooks/mnist_train.csv', header=None)

for row in df.itertuples():
      lable=row[0]
      bb=row[1]
      arr=numpy.asfarray(row[2:]).reshape(28,28)
      plt.title(f'label = {lable} bb={bb}')
      plt.imshow(arr, cmap='Greens',interpolation='hamming')
      break
# 好的, 这里能正常运行, 我们发现pandas在遍历行的时候, 他自动会把index7放到第一个元素, 咱们的lable成为了第二个元素, 要用[1]才能拿到, 现在, 咱们写个yield试试

# 2. yield写法, 单开一个cell做下面代码

def ite():
  for row in df.itertuples():
      lable=row[1]
      arr=numpy.asfarray(row[2:]).reshape(28,28)
      plt.title(f'label = {lable}')
      plt.imshow(arr, cmap='Greens',interpolation='hamming')
      yield
o=ite()

# 然后, 此时再新建一个cell, 敲入一下代码
next(o)
# ! 见证奇迹的时刻到了, 重复运行这个cell, 你会发现, 哇塞

```
#### 正常写准备数据的代码

```python




# 这里加载了pandas, dataset, plt
>>>>>>> Stashed changes
import pandas
import numpy
import torch
import torch.nn as nn
from torch.utils.data import Dataset
import matplotlib.pyplot as plt
df=pandas.read_csv('mount/My Drive/Colab Notebooks/mnist_train.csv', header=None)

for row in df.itertuples():
      lable=row[0]
      bb=row[1]
      arr=numpy.asfarray(row[2:]).reshape(28,28)
      plt.title(f'label = {lable} bb={bb}')
      plt.imshow(arr, cmap='Greens',interpolation='hamming')
      break
# 好的, 这里能正常运行, 我们发现pandas在遍历行的时候, 他自动会把index7放到第一个元素, 咱们的lable成为了第二个元素, 要用[1]才能拿到, 现在, 咱们写个yield试试

# 2. yield写法, 单开一个cell做下面代码

def ite():
  for row in df.itertuples():
      lable=row[1]
      arr=numpy.asfarray(row[2:]).reshape(28,28)
      plt.title(f'label = {lable}')
      plt.imshow(arr, cmap='Greens',interpolation='hamming')
      yield
o=ite()

# 然后, 此时再新建一个cell, 敲入一下代码
next(o)
# ! 见证奇迹的时刻到了, 重复运行这个cell, 你会发现, 哇塞

```
#### 正常写准备数据的代码

```python




# 这里加载了pandas, dataset, plt
>>>>>>> Stashed changes
import pandas
import numpy
import torch
import torch.nn as nn
from torch.utils.data import Dataset
import matplotlib.pyplot as plt
df=pandas.read_csv('mount/My Drive/Colab Notebooks/mnist_train.csv', header=None)

for row in df.itertuples():
      lable=row[0]
      bb=row[1]
      arr=numpy.asfarray(row[2:]).reshape(28,28)
      plt.title(f'label = {lable} bb={bb}')
      plt.imshow(arr, cmap='Greens',interpolation='hamming')
      break
# 好的, 这里能正常运行, 我们发现pandas在遍历行的时候, 他自动会把index7放到第一个元素, 咱们的lable成为了第二个元素, 要用[1]才能拿到, 现在, 咱们写个yield试试

# 2. yield写法, 单开一个cell做下面代码

def ite():
  for row in df.itertuples():
      lable=row[1]
      arr=numpy.asfarray(row[2:]).reshape(28,28)
      plt.title(f'label = {lable}')
      plt.imshow(arr, cmap='Greens',interpolation='hamming')
      yield
o=ite()

# 然后, 此时再新建一个cell, 敲入一下代码
next(o)
# ! 见证奇迹的时刻到了, 重复运行这个cell, 你会发现, 哇塞


# 3. 改进, 此时我们可以尝试写正常代码
  dftuple=df.itertuples()
  def ite():
    for row in dftuple:
      label=row[1]
      target=torch.zeros((10))
      target[label]=1.0
      img=torch.FloatTensor(row[2:])/255.0
      yield label, img, target
# ! 但是, 我们发现这个是有问题的, 咱们等于一开始就把pandas的dataframe转化为2维元组, 这个不科学, 更好的写法是逐个读入
  def ite():
    for i in range(len(df)) :
      label=df.iloc[i, 0]
      target=torch.zeros((10))
      target[label]=1.0
      img=torch.FloatTensor(df.iloc[i, 1:].values)/255.0
      yield label, img, target
  o=ite()
  # 单独一个cell
  print(next(o))


```
#### 正常写准备数据的代码

```python




# 这里加载了pandas, dataset, plt
import pandas
from torch.utils.data import Dataset
import matplotlib.pyplot as plt
del mdata
def mdata(csv_file):
  df=pandas.read_csv(csv_file, header=None)

  def ploti(index):
    arr=df.iloc[index, 1:].values.reshape(28,28)
    plt.title('label ='+str(df.iloc[index,0]))
    plt.imshow(arr, cmap='Greens',interpolation='hamming')
  mdata.ploti=ploti
<<<<<<< Updated upstream
<<<<<<< Updated upstream
=======

  dftuple=df.itertuples()
  def ite():
    for row in dftuple:
      label=row[1]
      target=torch.zeros((10))
      target[label]=1.0
      img=torch.FloatTensor(row[2:])/255.0
      yield label, img, target
  # ! 这里是yield函数, 因此要执行一下返回的是一个iterable, 这个functional是与众不同的, 因为他真的需要执行掉这个函数, 简单的说, 这里要的是函数返回的iterable对象
  mdata.list=ite()
>>>>>>> Stashed changes

  #mdata的长度
  mdata.len=len(df)
=======

  dftuple=df.itertuples()
  def ite():
    for row in dftuple:
      label=row[1]
      target=torch.zeros((10))
      target[label]=1.0
      img=torch.FloatTensor(row[2:])/255.0
      yield label, img, target
  # ! 这里是yield函数, 因此要执行一下返回的是一个iterable, 这个functional是与众不同的, 因为他真的需要执行掉这个函数, 简单的说, 这里要的是函数返回的iterable对象
  mdata.list=ite()
>>>>>>> Stashed changes

  # 定义一个得到某个元素的方法,
  def item(i):
      label=df.iloc[i, 0]
      target=torch.zeros((10))
      target[label]=1.0
      img=torch.FloatTensor(df.iloc[i, 1:].values)/255.0
      return label, img, target
  mdata.item=item
  def ite():
    for i in range(len(df)) :
      yield item(i)
  # ! 这里是yield函数, 因此要执行一下返回的是一个iterable, 这个functional是与众不同的, 因为他真的需要执行掉这个函数, 简单的说, 这里要的是函数返回的iterable对象
  mdata.list=ite()
  

# 测试下载入数据
mdata('mount/My Drive/Colab Notebooks/mnist_train.csv')
# 打印一个试试
mdata.ploti(8)
# 拿一组数据试试
print(next(mdata.list))
# 此时会打印一堆matrix数字
<<<<<<< Updated upstream
<<<<<<< Updated upstream
print(next(mdata.list)[0])
# 这个打印的就是当天一条代表的数字
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
```


### 训练代码
```python
# 训练网络
%%time
<<<<<<< Updated upstream
<<<<<<< Updated upstream
fnn() #? python没有立即执行函数表达式iife的概念, 因此, 函数必须被调用一次
=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
round=1
for i in range(round):
  print ('训练轮次:', i+1, '/总轮次',round)
  for label, image_data_tensor, target_tensor in mdata.list:
    fnn.train(image_data_tensor, target_tensor)
<<<<<<< Updated upstream
<<<<<<< Updated upstream
# 此时运行一下, 大家可以去透透气, 这个要点时间, 在我做实验的时候, 一个轮次要1分14秒
fnn.plot_progress()


=======
>>>>>>> Stashed changes
=======
>>>>>>> Stashed changes
```


### 测试验证代码

```python
mdata('mount/My Drive/Colab Notebooks/mnist_test.csv')
# 这个可以打印一个看看
mdata.ploti(19)

# 测试一个
output=fnn.forward(mdata.item(19)[1])

# 绘制这个输出张量
pandas.DataFrame(output.detach().numpy()).plot(kind='bar',legend=False, ylim=(0,1))
# 此处先把输出整理为numpy数组, 再转化为dataframe, 然后可以plot绘图


# --------测试计数代码---------
# 这里要思考下, 看看自己是否能独立写出来

count=0.0
for (label, image, target) in mdata.list:
  output=fnn.forward(image).detach().numpy()
  if label==output.argmax() :
    count+=1

print(count, mdata.len, count/mdata.len)


```



### 思考
- 最终依旧思考: 此时, 大家进行一个思考: 是什么原因, 导致我们直接神经网络逆向处理, 无法解决从概念到图形的问题的?
- 今天从原理上揭晓答案: 
 - 为什么从概念到图形这件事, 我们不能把神经网络倒过来用?
 - 因为: 我们没有合理的评价体系, 直接拟合多张图片, 那么等于拟合的是一个图片的统计值, 类比于:
  - 朴素唯物论
  - 庸俗物理学
  - 都是错的, 但是, 用了不同的情感词汇, 哲学家就是有政治家品质
 - 如何制作合理的评价体系?
- 请大家思考, 如何从原理上解决这个问题?