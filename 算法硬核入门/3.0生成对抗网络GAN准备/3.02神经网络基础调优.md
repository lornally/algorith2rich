
### 损失函数

```python
  # 定义损失函数, 此处使用均方误差, mean squared error
  mnn.loss_function=nn.MSELoss()
  # 均方差适合连续性任务, 比如预测温度, 预测降雨概率
  # 我们做的是分类任务, 需要同时惩罚错误的分类中的高数据和正确分类的低数据, 比如一个5, 最好5的概率是1, 其他数字的概率都是0
  # fixme 因此使用交叉熵binary cross entropy loss  BCELoss
  mnn.loss_function=nn.BCELoss()

# todo 此时尝试训练一下网络, 会发现明显改善

# ? 看一下训练图, 会让你大吃一惊
fnn.plot_progress()


```


### 激活函数

```python
# ! 饱和: s函数有个弱点, 如果输入值大了或者小了, 就会发生 [饱和saturation,] 也就是说网络对于输入的变化不再敏感, 输入无法通过梯度更新网络链接权重参数, 梯度消失了
nn.Sigmoid(), # 中间层的激活函数

# 有个选择, 线性函数rectified linear unit. ReLU(), 
# 不过这个函数左边小于〇的部分没有梯度, 我们还是需要一点梯度的, 因此使用泄露线性函数Leaky ReLU, LeakyReLU()
# ! 注意: 这货有参数
nn.LeakyReLU(0.02), # 中间层的激活函数
# ! 注意: 此时要把损失函数置回MSELoss, 不然损失函数会报错, 因为有负数了

```


> 此时可能遇到问题
> BCELoss配合LeakyReLU会报错, 输入必须在0-1之间, 因为 BCEloss不能配合leakyrelu

### 反向传播梯度更新
之前是随机梯度下降, 有两个问题
1. 陷入局部最小值
2. 所有参数都是同样的学习率
改为动量(momentum)方法, 可以一次性解决两个问题, Adam

```python
  # fixme 这里的动量momentum方案, 也不能配合leakyrelu/bceloss
  mnn.optimiser=torch.optim.SGD(mnn.parameters(), lr=0.01)

  # ! 注意参数数量变了, 他的学习率都不同, 所以不需要设置
  mnn.optimiser=torch.optim.Adam(mnn.parameters())
```


### 标准化(归一化)
参数和信号的值得问题
1. 取值范围大, 导致饱和, 学习困难
因此
1. 减少参数和信号的取值范围, 并且将均值转换为0
2. 在信号进入一个网络层之前, 进行标准化



```python
  nn.Linear(784,200), # 线性全连接从输入到中间 ax+b, 这里a和b都是学习会调整的参数, 此处已经比我们numpy的实现强大了一丢丢
      nn.LeakyReLU(0.02),   

      nn.LayerNorm(200), # ! 这里标准化了数据
      nn.Linear(200,10), # 线性全连接从中间到输出
      nn.LeakyReLU(0.02),


```


### 所有优化一起上
- 把所有的优化一起都用上

```python

# model调整了隐藏层激活函数, 做了输出层的标准化
mnn.model=nn.Sequential(
  nn.Linear(784,200), 
  nn.LeakyReLU(0.02),   # ! 隐藏层使用了relu
  nn.LayerNorm(200), # ! 这里标准化了数据
  nn.Linear(200,10), # 线性全连接从中间到输出
  nn.Sigmoid(), # fixme 因为BCE只能0-1, relu输出不止, 所以最终输出层不能用relu
)

# 损失函数换交叉熵
mnn.loss_function=nn.BCELoss()

# 梯度更新用动量方法 
# ! 注意参数数量变了, 他的学习率都不同, 所以不需要设置
mnn.optimiser=torch.optim.Adam(mnn.parameters())
```

- 训练一下, 只训练一轮, 效果让你大吃一惊


# leakyrelu不能配合bceloss, 文龙有个解决方案:
损失函数换成这个说不定就成了https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html